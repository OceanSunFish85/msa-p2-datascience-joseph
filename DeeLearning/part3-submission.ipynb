{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c1cb34",
   "metadata": {},
   "source": [
    "# MSA 2024 Phase 2 - Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa5f79",
   "metadata": {},
   "source": [
    "\n",
    "This notebook builds a Convolutional Neural Network (CNN) model for the CIFAR-10 dataset, with the use of `pytorch` to define the model structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f1c5d8",
   "metadata": {},
   "source": [
    "**Before start working on the competition, please ensure all required libraries are installed and properly set up on your system**:\n",
    "\n",
    "- `python >= 3.8`,\n",
    "- `pytorch >= 2.4.0`,\n",
    "- `CUDA 11.8`\n",
    "\n",
    "The following shows all the libraries that need to be referenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d550cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc85f3a2",
   "metadata": {},
   "source": [
    "### 1. Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49efe468",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset contains 60,000 images(32x32x3) in 10 different classes, with 6,000 images in each class. You can download the dataset directly from the competition webpage.\n",
    "\n",
    "Define data preprocessing and enhancement including on-the-fly horizontal inversion, random clipping padding, converting Tensor types and normalisation\n",
    "\n",
    "Define a dataset class to handle the reading of the data including finding the correct mapping of images to labels from train.csv\n",
    "\n",
    "Split the dataset, 80% for training, 10% for testing and %10 for validation, and set the random number generator seed to 101 to ensure consistency and reproducibility of the experimental results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3985907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data augmentation and preprocessing\n",
      "Loading training data\n",
      "Data loaded and split into train, validation, and test sets\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理和增强\n",
    "print(\"Data augmentation and preprocessing\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "    transforms.RandomCrop(32, padding=4),  # 随机裁剪并填充\n",
    "    transforms.ToTensor(),  # 转换为Tensor\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # 标准化\n",
    "])\n",
    "\n",
    "# 自定义数据集类\n",
    "class CIFAR10Dataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, f\"image_{self.data_frame.iloc[idx, 0]}.png\")\n",
    "        image = Image.open(img_name).convert('RGB')  # 确保图像是RGB格式\n",
    "        label = int(self.data_frame.iloc[idx, 1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "# 加载训练数据\n",
    "print(\"Loading training data\")\n",
    "dataset = CIFAR10Dataset(csv_file='nzmsa-2024/train.csv', root_dir='nzmsa-2024/cifar10_images/train', transform=transform)\n",
    "\n",
    "# 拆分数据集为训练集、验证集和测试集\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(101))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Data loaded and split into train, validation, and test sets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261ae9e8",
   "metadata": {},
   "source": [
    "### 2. Build & train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f0331",
   "metadata": {},
   "source": [
    "The model includes six convolutional layers with different numbers of channels to gradually extract different levels of features from the input image. Each convolution is followed by a normalization layer to stabilize and accelerate training, improving model stability. Additionally, the spatial dimensions of the feature map are gradually reduced using a maximum pooling layer to focus on the most significant features. \n",
    "\n",
    "1. **Convolutional Layers**:\n",
    "   - **Purpose**: Extract hierarchical features from the input image.\n",
    "   - **Details**: Six convolutional layers with increasing channels to capture different levels of details.\n",
    "\n",
    "2. **Batch Normalization Layers**:\n",
    "   - **Purpose**: Stabilize and accelerate training.\n",
    "   - **Details**: Placed after each convolutional layer to normalize the output, reducing internal covariate shift.\n",
    "\n",
    "3. **Max Pooling Layers**:\n",
    "   - **Purpose**: Reduce the spatial dimensions of feature maps.\n",
    "   - **Details**: Used to focus on the most significant features and reduce computational complexity.\n",
    "\n",
    "4. **Fully Connected Layers**:\n",
    "   - **Purpose**: Map high-dimensional features to the final output.\n",
    "   - **Details**: Connect the flattened feature map from convolutional and pooling layers to the output layer for classification.\n",
    "\n",
    "5. **Dropout Layers**:\n",
    "   - **Purpose**: Reduce overfitting and enhance generalization.\n",
    "   - **Details**: Randomly discard neurons during training to prevent overfitting.\n",
    "\n",
    "6. **ReLU Activation Function**:\n",
    "   - **Purpose**: Introduce nonlinearity.\n",
    "   - **Details**: Applied after convolutional and fully connected layers to enable the network to learn and represent complex nonlinear features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d072d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, conv1_channels=64, conv2_channels=128, conv3_channels=256, conv4_channels=512, fc1_size=1024, fc2_size=512, dropout_p=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=conv1_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(conv1_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv1_channels, out_channels=conv2_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(conv2_channels)\n",
    "        self.conv3 = nn.Conv2d(in_channels=conv2_channels, out_channels=conv3_channels, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(conv3_channels)\n",
    "        self.conv4 = nn.Conv2d(in_channels=conv3_channels, out_channels=conv4_channels, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(conv4_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=conv4_channels, out_channels=conv4_channels, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(conv4_channels)\n",
    "        self.conv6 = nn.Conv2d(in_channels=conv4_channels, out_channels=conv4_channels, kernel_size=3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(conv4_channels)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.fc1 = nn.Linear(conv4_channels * 2 * 2, fc1_size)\n",
    "        self.bn7 = nn.BatchNorm1d(fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.bn8 = nn.BatchNorm1d(fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool(F.relu(self.bn6(self.conv6(x))))\n",
    "        \n",
    "        x = x.view(-1, self.conv4.out_channels * 2 * 2)\n",
    "        x = F.relu(self.bn7(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn8(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862100bc",
   "metadata": {},
   "source": [
    "Create functions to save and load breakpoints for breakpoint training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea150cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='models/checkpoint.pth.tar'):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True) \n",
    "    torch.save(state, filename)\n",
    "    print(f\"Checkpoint saved at {filename}\")\n",
    "\n",
    "def load_checkpoint(filename, model, optimizer):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Checkpoint loaded from {filename}, starting from epoch {epoch}\")\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ef8e5",
   "metadata": {},
   "source": [
    "Define training and validation functions and plot learning curves for initial evaluation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad717279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练和验证过程\n",
    "def train_and_validate(net, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, start_epoch=0, early_stopping_patience=5):\n",
    "    scaler = GradScaler()\n",
    "    best_val_accuracy = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print(f'[Epoch {epoch + 1}, Iter {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # 验证模型\n",
    "        net.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with autocast():\n",
    "                    outputs = net(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        scheduler.step(val_loss / len(val_loader))\n",
    "        print(f'[Epoch {epoch + 1}] validation loss: {val_loss / len(val_loader):.3f}, accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # 保存模型断点\n",
    "        is_best = val_accuracy > best_val_accuracy\n",
    "        if is_best:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, filename='models/best_model.pth.tar')\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    print('Finished Training')\n",
    "    \n",
    "    # 绘制学习曲线\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9789fa6",
   "metadata": {},
   "source": [
    "Apply the model class and set up the loss function and optimiser. \n",
    "The AdamW optimiser is used, which features decoupling of weight decay and gradient updating to obtain better generalisation while adaptively adjusting the learning rate based on the first-order moment estimate and second-order moment estimate of the gradient. \n",
    "The learning rate is also dynamically adjusted using the ReduceLROnPlateau learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a22cc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(\n",
    "    num_classes=10,\n",
    "    conv1_channels=64,\n",
    "    conv2_channels=128,\n",
    "    conv3_channels=256,\n",
    "    conv4_channels=512,\n",
    "    fc1_size=1024,\n",
    "    fc2_size=512,\n",
    "    dropout_p=0.5\n",
    ").to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da93b7e",
   "metadata": {},
   "source": [
    "Perform model training and validation\n",
    "Perform 30 epoches of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d33c824a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training and validation\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 检查设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 加载模型断点（如果有）\n",
    "    start_epoch = 0\n",
    "    checkpoint_path = 'models/best_model.pth.tar'\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        start_epoch = load_checkpoint(checkpoint_path, model, optimizer)\n",
    "\n",
    "    # 训练和验证模型\n",
    "    print(\"Starting training and validation\")\n",
    "    train_and_validate(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30, device=device, start_epoch=start_epoch, early_stopping_patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433d30a",
   "metadata": {},
   "source": [
    "## 3. Evaluate Models\n",
    "\n",
    "Setting the confusion matrix and ROC plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286992ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix'):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(labels, preds, num_classes):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    labels = np.array(labels)\n",
    "    preds = np.array(preds)\n",
    "    for i in range(num_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels == i, preds == i)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i in range(num_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f'Class {i} (area = {roc_auc[i]:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc767c41",
   "metadata": {},
   "source": [
    "Evaluation results are saved in the results folder including confusion matrix, ROC, and other evaluation values are saved in evaluate.csv\n",
    "\n",
    "#### Evaluation criteria used\n",
    "\n",
    "- **Accuracy**: indicates the proportion of the test set that the model correctly classifies.\n",
    "- **precision**: indicates the proportion of all samples that were predicted by the model to be positively classified that were actually positively classified.\n",
    "- **recall**: indicates the proportion of all samples that were actually positively classified by the model that were correctly predicted to be positively classified by the model.\n",
    "- **f1-score**: the reconciled mean of precision and recall.\n",
    "- **support**: the number of times each category appears in the dataset.\n",
    "\n",
    "#### Detailed Analysis\n",
    "\n",
    "1. **Training set vs. test set accuracy**:\n",
    "   - **Training Set Accuracy**: 0.960775 shows that the model has high accuracy on the training set, indicating that the model learnt the data patterns of the training set well.\n",
    "   - **Test Set Accuracy**: 0.8784 decreased relative to the training set accuracy, but still maintained at a high level, indicating that the model has some generalisation ability.\n",
    "\n",
    "2. **Precision, recall and f1-score**:\n",
    "   - **Training set**: precision, recall and f1-score for all categories are close to or above 0.90, indicating that the model has almost no misclassification on the training set.\n",
    "   - **Test set**: Precision, recall and f1-score of most categories are above 0.80, indicating that the model also performs more consistently on the test set. However, there is a slight decrease in some of the categories' metrics relative to the training set, especially for category 0 and category 5, showing that the model's performance on these categories needs to be improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36551f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估最佳模型在训练集和测试集上的表现\n",
    "print(\"Evaluating the best model on the training set\")\n",
    "train_labels, train_preds = evaluate_model(model, train_loader, device, num_classes=10)\n",
    "print(\"Training Accuracy: \", accuracy_score(train_labels, train_preds))\n",
    "print(\"Classification Report for Training Set:\\n\", classification_report(train_labels, train_preds))\n",
    "\n",
    "print(\"Evaluating the best model on the test set\")\n",
    "test_labels, test_preds = evaluate_model(model, test_loader, device, num_classes=10)\n",
    "print(\"Test Accuracy: \", accuracy_score(test_labels, test_preds))\n",
    "print(\"Classification Report for Test Set:\\n\", classification_report(test_labels, test_preds))\n",
    "\n",
    "# 混淆矩阵\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plot_confusion_matrix(cm, classes=[f'Class {i}' for i in range(10)], title='Confusion Matrix for Test Set')\n",
    "\n",
    "# ROC 曲线\n",
    "plot_roc_curve(test_labels, test_preds, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725025e3",
   "metadata": {},
   "source": [
    "**ROC Curve**\n",
    "\n",
    "- **ROC Curve**: Shows the trade-off between the True Positive Rate (TPR) and the False Positive Rate (FPR) of the model at various thresholds.\n",
    "- **AUC (Area Under Curve)**: The area under the ROC curve, ranging from 0 to 1, with larger values indicating better model performance.\n",
    "  - The AUC values are all around 0.90, indicating that the model has good prediction performance on most classes, especially the AUC value of 0.97 for Class 3, which is the best performance.\n",
    "\n",
    "**Confusion Matrix**\n",
    "\n",
    "- **True Positives (TP)**: Values on the diagonal indicating the number of samples correctly classified.\n",
    "- **False Positives (FP)**: Values of columns not on the diagonal, indicating the number of samples incorrectly categorised as that class.\n",
    "- **False Negatives (FN)**: Non-diagonal row values indicating the number of samples that were incorrectly classified as other classes for that class.\n",
    "\n",
    "**Analysis**:\n",
    "- The number of correctly classified samples is high for most classes.\n",
    "- **Class 0** and **Class 5** have significant misclassification problems and need further optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3ea4b",
   "metadata": {},
   "source": [
    "## 4. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef3cdc",
   "metadata": {},
   "source": [
    "Creating predictive functions, including using trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b390ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            results.extend(zip(image_ids.cpu().numpy(), predicted.cpu().numpy()))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc008c",
   "metadata": {},
   "source": [
    "Load dataset and test, save predictions to predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078755ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预测数据\n",
    "predict_dataset = CIFAR10Dataset(root_dir='nzmsa-2024/cifar10_images/test', transform=transform)\n",
    "predict_loader = DataLoader(predict_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "# 检查预测数据加载是否正确\n",
    "print(\"Checking loaded prediction data...\")\n",
    "for images, image_ids in predict_loader:\n",
    "    print(f\"Image IDs: {image_ids[:5]}\")\n",
    "    print(f\"Image tensor shape: {images.shape}\")\n",
    "    break  # 只检查一个batch\n",
    "\n",
    "# 进行预测\n",
    "results = predict(model, predict_loader, device)\n",
    "\n",
    "# 保存结果到CSV文件\n",
    "results_df = pd.DataFrame(results, columns=['image_id', 'label'])\n",
    "results_df.sort_values(by='image_id', inplace=True)  # 按image_id排序\n",
    "results_df.to_csv('predictions.csv', index=False)\n",
    "print(\"Predictions saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142fd80c",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "In this challenge, we built a convolutional neural network with a convolutional layer, a normalisation layer, a pooling layer, a fully connected layer, a dropout layer, and an activation function, and performed a comprehensive evaluation of its training results.\n",
    "\n",
    "#### Model Evaluation\n",
    "\n",
    "- **Accuracy**:\n",
    "  - The accuracy of the model remained high on both the training and test sets. The test set accuracy was lower than the training set, but also reached 87%.\n",
    "\n",
    "- **Precision, Recall and f1 Score**:\n",
    "  - Precision, recall and f1 scores remain high on both the training and test sets. The evaluation results were close to 90% for the training set and greater than 80% for the test set, but the model's metrics dropped slightly on Classification 0 and Classification 5 on the test set.\n",
    "\n",
    "- **Confusion matrix and ROC curve**:\n",
    "  - By analysing the results, the AUC values in the ROC curves are all around 90%, which means that the model has good prediction ability for most of the categories.\n",
    "  - In the confusion matrix, we find that the true positives of most categories are high, which means that the model has a good ability to correctly classify, but misclassification occurs in category 0 and category 5, which needs further optimisation.\n",
    "\n",
    "#### Final Results\n",
    "- We made predictions on the test dataset for the challenge and saved the results as ID and Label in the `predication.csv` file.\n",
    "\n",
    "### Model Optimisation Recommendations\n",
    "\n",
    "Based on the above evaluation, we can optimise the model as follows:\n",
    "\n",
    "1. **Data Enhancement**:\n",
    "   - Increase data diversity and reduce the risk of overfitting.\n",
    "\n",
    "2. **Adjustment of model structure**:\n",
    "   - Increase the number of convolutional layers or increase the number of neurons to improve the model's ability to recognise complex patterns.\n",
    "\n",
    "3. **Tuning hyperparameters**:\n",
    "   - Find the optimal model parameters to improve model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
